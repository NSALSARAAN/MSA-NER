{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dP-0hZWKhOFQ",
    "outputId": "5de277e7-5ba4-4cbf-8db8-77549e13f1fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
      "\r",
      "\u001b[K     |▎                               | 10kB 23.6MB/s eta 0:00:01\r",
      "\u001b[K     |▌                               | 20kB 32.2MB/s eta 0:00:01\r",
      "\u001b[K     |▊                               | 30kB 35.6MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 40kB 24.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▎                              | 51kB 15.6MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 61kB 14.4MB/s eta 0:00:01\r",
      "\u001b[K     |█▊                              | 71kB 13.0MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 81kB 13.7MB/s eta 0:00:01\r",
      "\u001b[K     |██▎                             | 92kB 14.1MB/s eta 0:00:01\r",
      "\u001b[K     |██▌                             | 102kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 112kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 122kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███▎                            | 133kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 143kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 153kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 163kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████▎                           | 174kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████▌                           | 184kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 194kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 204kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 215kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 225kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████▉                          | 235kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 245kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 256kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████▌                         | 266kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████▉                         | 276kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 286kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 296kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 307kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 317kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 327kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 337kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 348kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████▉                       | 358kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 368kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▎                      | 378kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 389kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▉                      | 399kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 409kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 419kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 430kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 440kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 450kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 460kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▋                    | 471kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▉                    | 481kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 491kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 501kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 512kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 522kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 532kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 542kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▋                  | 552kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▉                  | 563kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 573kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 583kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 593kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 604kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 614kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▍                | 624kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 634kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 645kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▏               | 655kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 665kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 675kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▉               | 686kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 696kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▍              | 706kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 716kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 727kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▏             | 737kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 747kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 757kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 768kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▏            | 778kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 788kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 798kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 808kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 819kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 829kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 839kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 849kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 860kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▍          | 870kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 880kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 890kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 901kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 911kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 921kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 931kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 942kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 952kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▊        | 962kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 972kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 983kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 993kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 1.0MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 1.0MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 1.0MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 1.0MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 1.0MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 1.1MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 1.1MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 1.1MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▊     | 1.1MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 1.1MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▏    | 1.1MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 1.1MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 1.1MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 1.1MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 1.1MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 1.2MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▊   | 1.2MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 1.2MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 1.2MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 1.2MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 1.2MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 1.2MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 1.2MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 1.2MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 1.2MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 1.3MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▎| 1.3MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 1.3MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▊| 1.3MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.3MB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.3MB 12.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Collecting tokenizers==0.9.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 57.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Collecting sentencepiece==0.1.91\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 49.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 53.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=177a34470ee15095f2e968d706bf8f6e98d3b60683e053f5ee0b40e847e21754\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oZ30Sbz5RPXU",
    "outputId": "0dd6574b-bfb8-45d9-8e84-568cd28f3603"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seqeval\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
      "\r",
      "\u001b[K     |███████▌                        | 10kB 22.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 20kB 27.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 30kB 24.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 40kB 18.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 51kB 5.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.22.2.post1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (0.17.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-cp36-none-any.whl size=16171 sha256=bed7e4bf5300318d532f7a290ad4b11f08cfc2c6b360f4b8275c302798fe172b\n",
      "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n"
     ]
    }
   ],
   "source": [
    "pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ljlZy6SV4AkP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o52iq5dUhCzs",
    "outputId": "18ecf7bf-c4d0-4d8a-f9aa-c93702069c0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n",
      "Thu Nov 19 13:20:28 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   38C    P0    26W / 250W |     10MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                 ERR! |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    !nvidia-smi\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "D5dwnfw7g-rm"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "dump_chars = '!\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~’،ـ؟؛«» '\n",
    "\n",
    "def clean_word(word):\n",
    "    word = word.translate(str.maketrans({key: None for key in dump_chars}))\n",
    "    \n",
    "    #remove tashkeel\n",
    "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
    "    word = re.sub(p_tashkeel,\"\", word)\n",
    "    \n",
    "    return word\n",
    "\n",
    "def load_data(filename):\n",
    "    f = open(filename, 'r',encoding ='utf-8')\n",
    "    sents = f.read().split('\\n. O\\n')\n",
    "    f.close()\n",
    "    # tokenize words\n",
    "    words = [None]*len(sents)\n",
    "    tokens = [None]*len(sents)\n",
    "    for i, sent in enumerate(sents):\n",
    "        sent = sent.split('\\n')\n",
    "        words[i] = []\n",
    "        tokens[i] = []\n",
    "        for word in sent:\n",
    "            line = word.rsplit(' ', 1)\n",
    "            line[0] = clean_word(line[0])\n",
    "            if len(line[0]) > 0:\n",
    "                words[i].append(line[0])\n",
    "                tokens[i].append(line[1])\n",
    "                    \n",
    "                \n",
    "    return [d for d in words if len(d) > 0], [d for d in tokens if len(d) > 0]\n",
    "\n",
    "# load data\n",
    "tag_classes = ['O','B-LOC', 'B-MISC', 'B-ORG', 'B-PERS', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PERS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hxY2VPnROAjg"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Asents, Alabels = load_data ('ANERCorp')\n",
    "Atrain_sents, Atest_sents, Atrain_labels, Atest_labels = train_test_split(Asents, Alabels, test_size=0.1, random_state=2018)\n",
    "Atrain_sents, Aval_sents, Atrain_labels, Aval_labels = train_test_split(Atrain_sents, Atrain_labels, test_size=0.1, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OSFYFBbbOUb0"
   },
   "outputs": [],
   "source": [
    "Qsents, Qlabels = load_data ('AQMAR')\n",
    "Qtrain_sents, Qtest_sents, Qtrain_labels, Qtest_labels = train_test_split(Qsents, Qlabels, test_size=0.1, random_state=2018)\n",
    "Qtrain_sents, Qval_sents, Qtrain_labels, Qval_labels = train_test_split(Qtrain_sents, Qtrain_labels, test_size=0.1, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Eqj4a00Rv32X"
   },
   "outputs": [],
   "source": [
    "with open(\"ANERCorp_train.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i , row in enumerate (Atrain_sents):\n",
    "        for word, label in zip(row, Atrain_labels[i]):\n",
    "            f.write(word + \" \" + label + \"\\n\")\n",
    "        f.write(\". O\\n\")\n",
    "with open(\"ANERCorp_test.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i , row in enumerate (Atest_sents):\n",
    "        for word, label in zip(row, Atest_labels[i]):\n",
    "            f.write(word + \" \" + label + \"\\n\")\n",
    "        f.write(\". O\\n\")\n",
    "with open(\"ANERCorp_val.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i , row in enumerate (Aval_sents):\n",
    "        for word, label in zip(row, Aval_labels[i]):\n",
    "            f.write(word + \" \" + label + \"\\n\")\n",
    "        f.write(\". O\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IT_huz0RxBUb"
   },
   "outputs": [],
   "source": [
    "with open(\"AQMAR_train.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i , row in enumerate (Qtrain_sents):\n",
    "        for word, label in zip(row, Qtrain_labels[i]):\n",
    "            f.write(word + \" \" + label + \"\\n\")\n",
    "        f.write(\". O\\n\")\n",
    "with open(\"AQMAR_test.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i , row in enumerate (Qtest_sents):\n",
    "        for word, label in zip(row, Qtest_labels[i]):\n",
    "            f.write(word + \" \" + label + \"\\n\")\n",
    "        f.write(\". O\\n\")\n",
    "with open(\"AQMAR_val.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i , row in enumerate (Qval_sents):\n",
    "        for word, label in zip(row, Qval_labels[i]):\n",
    "            f.write(word + \" \" + label + \"\\n\")\n",
    "        f.write(\". O\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rmldOCIbtMfK"
   },
   "outputs": [],
   "source": [
    "Atrain_sents, Atrain_labels = load_data ('ANERCorp_train.tsv')\n",
    "Atest_sents, Atest_labels = load_data ('ANERCorp_test.tsv')\n",
    "Aval_sents, Aval_labels  = load_data ('ANERCorp_val.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "A4VZGzfNvMKm"
   },
   "outputs": [],
   "source": [
    "\n",
    "Qtrain_sents, Qtrain_labels = load_data ('AQMAR_train.tsv')\n",
    "Qtest_sents, Qtest_labels = load_data ('AQMAR_test.tsv')\n",
    "Qval_sents, Qval_labels  = load_data ('AQMAR_val.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1gQUF41xqb6W",
    "outputId": "2da8ec52-8aef-41e6-8af0-6ea07b753d1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'O': 96263, 'B-LOC': 3616, 'B-PERS': 2869, 'I-PERS': 2275, 'B-ORG': 1617, 'I-ORG': 1134, 'B-MISC': 901, 'I-LOC': 508, 'I-MISC': 418})\n",
      "Counter({'O': 12361, 'B-LOC': 431, 'B-PERS': 417, 'I-PERS': 308, 'B-ORG': 239, 'I-ORG': 152, 'B-MISC': 106, 'I-MISC': 58, 'I-LOC': 56})\n",
      "Counter({'O': 10559, 'B-LOC': 383, 'B-PERS': 317, 'I-PERS': 251, 'B-ORG': 170, 'B-MISC': 110, 'I-ORG': 96, 'I-MISC': 62, 'I-LOC': 40})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter([ label for labels in Atrain_labels for label in labels]))\n",
    "print(Counter([ label for labels in Atest_labels for label in labels]))\n",
    "print(Counter([ label for labels in Aval_labels for label in labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-5RWLB3Gv5aC",
    "outputId": "49138061-b39c-4518-b0ce-f2d779922f87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'O': 44055, 'B-MISC': 2033, 'B-PERS': 1127, 'I-MISC': 1115, 'B-LOC': 1045, 'I-PERS': 691, 'I-LOC': 449, 'I-ORG': 351, 'B-ORG': 322})\n",
      "Counter({'O': 5782, 'B-MISC': 256, 'B-PERS': 177, 'I-MISC': 151, 'B-LOC': 149, 'I-PERS': 115, 'I-LOC': 65, 'B-ORG': 45, 'I-ORG': 42})\n",
      "Counter({'O': 5567, 'B-MISC': 185, 'B-PERS': 150, 'B-LOC': 136, 'I-PERS': 117, 'I-MISC': 94, 'I-LOC': 55, 'B-ORG': 27, 'I-ORG': 27})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(Counter([ label for labels in Qtrain_labels for label in labels]))\n",
    "print(Counter([ label for labels in Qtest_labels for label in labels]))\n",
    "print(Counter([ label for labels in Qval_labels for label in labels]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LK0J5csmyf6X",
    "outputId": "970bf860-2c53-4800-d3c4-2d0039a2417b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANERCorp\n",
      "labels --- train --- test --- val\n",
      "other ---  96263 --- 12361 --- 10559\n",
      "person ---  5144 --- 725 --- 568\n",
      "location ---  4124 --- 487 --- 423\n",
      "organization ---  2751 --- 391 --- 266\n",
      "MISC ---  1319 --- 164 --- 172\n",
      "AQMAR\n",
      "labels --- train --- test --- val\n",
      "other ---  44055 --- 5782 --- 5567\n",
      "person ---  1818 --- 292 --- 267\n",
      "location ---  1494 --- 214 --- 191\n",
      "organization ---  673 --- 87 --- 54\n",
      "MISC ---  3148 --- 407 --- 279\n"
     ]
    }
   ],
   "source": [
    "#----------------count labels------------------\n",
    "print ('ANERCorp')\n",
    "x = Counter([ label for labels in Atrain_labels for label in labels])\n",
    "y = Counter([ label for labels in Atest_labels for label in labels])\n",
    "z = Counter([ label for labels in Aval_labels for label in labels])\n",
    "print ('labels --- train --- test --- val')\n",
    "print ('other --- ' , x['O'] , '---' ,y['O'], '---' , z['O'])\n",
    "print ('person --- ' , x['B-PERS']+x['I-PERS'] , '---' ,y['B-PERS']+y['I-PERS'], '---' , z['B-PERS']+z['I-PERS'])\n",
    "print ('location --- ' , x['B-LOC']+x['I-LOC'] , '---' ,y['B-LOC']+y['I-LOC'], '---' , z['B-LOC']+z['I-LOC'])\n",
    "print ('organization --- ' , x['B-ORG']+x['I-ORG'] , '---' ,y['B-ORG']+y['I-ORG'], '---' , z['B-ORG']+z['I-ORG'])\n",
    "print ('MISC --- ' , x['B-MISC']+x['I-MISC'] , '---' ,y['B-MISC']+y['I-MISC'], '---' , z['B-MISC']+z['I-MISC'])\n",
    "\n",
    "print ('AQMAR')\n",
    "x = Counter([ label for labels in Qtrain_labels for label in labels])\n",
    "y = Counter([ label for labels in Qtest_labels for label in labels])\n",
    "z = Counter([ label for labels in Qval_labels for label in labels])\n",
    "print ('labels --- train --- test --- val')\n",
    "print ('other --- ' , x['O'] , '---' ,y['O'], '---' , z['O'])\n",
    "print ('person --- ' , x['B-PERS']+x['I-PERS'] , '---' ,y['B-PERS']+y['I-PERS'], '---' , z['B-PERS']+z['I-PERS'])\n",
    "print ('location --- ' , x['B-LOC']+x['I-LOC'] , '---' ,y['B-LOC']+y['I-LOC'], '---' , z['B-LOC']+z['I-LOC'])\n",
    "print ('organization --- ' , x['B-ORG']+x['I-ORG'] , '---' ,y['B-ORG']+y['I-ORG'], '---' , z['B-ORG']+z['I-ORG'])\n",
    "print ('MISC --- ' , x['B-MISC']+x['I-MISC'] , '---' ,y['B-MISC']+y['I-MISC'], '---' , z['B-MISC']+z['I-MISC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0G7K_h4KNE3m"
   },
   "outputs": [],
   "source": [
    "train_sents = Atrain_sents + Qtrain_sents\n",
    "test_sents = Atest_sents + Qtest_sents\n",
    "val_sents = Aval_sents + Qval_sents\n",
    "\n",
    "\n",
    "train_labels = Atrain_labels + Qtrain_labels\n",
    "test_labels = Atest_labels + Qtest_labels\n",
    "val_labels = Aval_labels + Qval_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bsv_avvR46df",
    "outputId": "9352794a-f402-4bb4-ae26-325a5a06e506"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels --- train --- test --- val\n",
      "other ---  140318 --- 18143 --- 16126\n",
      "person ---  6962 --- 1017 --- 835\n",
      "location ---  5618 --- 701 --- 614\n",
      "organization ---  3424 --- 478 --- 320\n",
      "MISC ---  4467 --- 571 --- 451\n"
     ]
    }
   ],
   "source": [
    "x = Counter([ label for labels in train_labels for label in labels])\n",
    "y = Counter([ label for labels in test_labels for label in labels])\n",
    "z = Counter([ label for labels in val_labels for label in labels])\n",
    "print ('labels --- train --- test --- val')\n",
    "print ('other --- ' , x['O'] , '---' ,y['O'], '---' , z['O'])\n",
    "print ('person --- ' , x['B-PERS']+x['I-PERS'] , '---' ,y['B-PERS']+y['I-PERS'], '---' , z['B-PERS']+z['I-PERS'])\n",
    "print ('location --- ' , x['B-LOC']+x['I-LOC'] , '---' ,y['B-LOC']+y['I-LOC'], '---' , z['B-LOC']+z['I-LOC'])\n",
    "print ('organization --- ' , x['B-ORG']+x['I-ORG'] , '---' ,y['B-ORG']+y['I-ORG'], '---' , z['B-ORG']+z['I-ORG'])\n",
    "print ('MISC --- ' , x['B-MISC']+x['I-MISC'] , '---' ,y['B-MISC']+y['I-MISC'], '---' , z['B-MISC']+z['I-MISC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "huUcTbCtgChe",
    "outputId": "fe8f808d-7551-48c0-c466-fca59b7fa087"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ولم', 'يعد', 'من', 'الممكن', 'تبرير', 'سلوك', 'القوى', 'السياسية', 'الشيعية', 'للعرب', 'والمسلمين', 'لا', 'من', 'حيث', 'تحالف', 'هذه', 'القوى', 'مع', 'الاحتلال', 'وتصديها', 'للمقاومة', 'ولا', 'من', 'حيث', 'سياستها', 'وسلوكها', 'الطائفي']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print (test_sents[0])\n",
    "print (test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "weK7SWOe1u4o",
    "outputId": "218fc92b-57b3-48fe-ecce-0d4c4d7d1dbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max train sentence length: 176\n",
      "Max val sentence length: 212\n",
      "Max test sentence length: 167\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Max train sentence length:',len(max(train_sents, key=len)))\n",
    "print('Max val sentence length:',len(max(val_sents, key=len)))\n",
    "print('Max test sentence length:',len(max(test_sents, key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "kTYv2VUsdbaV"
   },
   "outputs": [],
   "source": [
    "def split(sentences,labels, max):\n",
    "    news = []\n",
    "    newl = []\n",
    "    for i , sent in enumerate (sentences):\n",
    "      tag = labels[i]\n",
    "      if len (sent) < max:\n",
    "        news.append(sent)\n",
    "        newl.append(tag)\n",
    "      else:\n",
    "        x = max \n",
    "        while (len (sent) > 0):\n",
    "          if len(sent) < x :\n",
    "            news.append(sent)\n",
    "            newl.append(tag)\n",
    "            sent = []\n",
    "          else: \n",
    "            if tag[x-1]=='O' or tag[x-1]==0 or tag [x] == 'O' or tag [x] == 0:\n",
    "              news.append(sent[:x])\n",
    "              newl.append(tag[:x])\n",
    "              sent = sent[x:]\n",
    "              tag = tag [x:]\n",
    "              x = max\n",
    "            else:\n",
    "              x = x-1\n",
    "    return news, newl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nyfYBxqwU3RN",
    "outputId": "83f3c05d-a8db-49c5-92d2-a2204927f551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max train sentence length: 100\n",
      "Max val sentence length: 100\n",
      "Max test sentence length: 100\n"
     ]
    }
   ],
   "source": [
    "train_sents,train_labels = split(train_sents,train_labels, 100)\n",
    "test_sents,test_labels = split(test_sents,test_labels, 100)\n",
    "val_sents,val_labels = split(val_sents,val_labels, 100)\n",
    "\n",
    "\n",
    "print('Max train sentence length:',len(max(train_sents, key=len)))\n",
    "print('Max val sentence length:',len(max(val_sents, key=len)))\n",
    "print('Max test sentence length:',len(max(test_sents, key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "cICIt2AGXOkw"
   },
   "outputs": [],
   "source": [
    "bert_model = 'aubmindlab/bert-base-arabertv01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 132,
     "referenced_widgets": [
      "30ee8c72ceda4b969afbb6743d2d254e",
      "3398b97e49de4f74b50911fe8fbdf18d",
      "6ee106c686ae403a87bf69dd671ef59e",
      "a8e5fe7736e6469d93aaddc875900087",
      "accea898bce140eb87e648a4bd437bac",
      "f5d00c478fce408e93e217c0bac88c5f",
      "b1175768036e4d94b1eee31a14654e9b",
      "fd0c20fc35bb49f18302c97785df0fff",
      "ad53b5be46e54a218ca8807661b5d7bf",
      "0d76b4f0d64a4b83a6c129c2bb2d6dd6",
      "1c51d1d5c7144a8e905946f60cc28b0d",
      "dafa709a606a41249c7534f15d678f6c",
      "bd5e04057060457882459661e3277830",
      "3ab439891e0046cea393cea902283a40",
      "5a198de7545e4bf091d9c30492304088",
      "d5fa2aa6a9db4fd6b946e1a6e59270ce"
     ]
    },
    "id": "yq807jlKiJ-5",
    "outputId": "1c9b8812-5abb-47b5-bdb1-0f430427688d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30ee8c72ceda4b969afbb6743d2d254e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad53b5be46e54a218ca8807661b5d7bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=780030.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer,AutoTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model,do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "yqwJ46-iABjK",
    "outputId": "aa47b95f-06b2-4e9a-f1d8-32a820a23128"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n      word_tokens = tokenizer.tokenize(word)\\n      max = 0\\n      index = 0 \\n      for k, t in enumerate (word_tokens):\\n        if (len (t) > max): \\n          max = len (t)\\n          index = k\\n      tokenized_word.extend (word_tokens)\\n      for k, t in enumerate (word_tokens):\\n        if (k == index): \\n          tokenized_label.append(labels[i][j])\\n        else:\\n          tokenized_label.append(0)\\n  '"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 256\n",
    "#------------------------sents processing --------------------------\n",
    "#-------------------keep all tokens--------------\n",
    "\"\"\"\n",
    "def encode_sentence(sents,labels):\n",
    "  tokenized_sents = []\n",
    "  tokenized_labels = []\n",
    "\n",
    "  for i , sent in enumerate (sents):\n",
    "    tokenized_word = []\n",
    "    tokenized_label = []\n",
    "    tokenized_word.append(\"[CLS]\")\n",
    "    tokenized_label.append(0)\n",
    "    for j , word in enumerate (sent):\n",
    "      word_tokens = tokenizer.tokenize(word)\n",
    "      if len(word_tokens) > 0:\n",
    "        tokenized_word.extend(word_tokens)\n",
    "        # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "        tokenized_label.extend([labels[i][j]] + [0] * (len(word_tokens) - 1))\n",
    "    tokenized_word.append(\"[SEP]\")\n",
    "    tokenized_label.append(0)\n",
    "    special_tokens_count = tokenizer.num_special_tokens_to_add()\n",
    "    if len(tokenized_word) > max_len - special_tokens_count:\n",
    "        tokenized_word = tokenized_word[: (max_len - special_tokens_count)]\n",
    "        tokenized_label = tokenized_label[: (max_len - special_tokens_count)]    \n",
    "    tokenized_sents.append(tokenized_word)\n",
    "    tokenized_labels.append(tokenized_label)\n",
    "\n",
    "  \n",
    "  return tokenized_sents, tokenized_labels\n",
    "\"\"\"\n",
    "\n",
    "def encode_sentence(sents,labels):\n",
    "  tokenized_sents = []\n",
    "  tokenized_labels = []\n",
    "  Fasttext_seq = []\n",
    "  for i , sent in enumerate (sents):\n",
    "    tokenized_word = []\n",
    "    tokenized_label = []\n",
    "    #word_seq = []\n",
    "    tokenized_word.append(\"[CLS]\")\n",
    "    tokenized_label.append(0)\n",
    "    #word_seq.append(0)\n",
    "    for j , word in enumerate (sent):\n",
    "      word_tokens = tokenizer.tokenize(word)\n",
    "      tokenized_word.extend (word_tokens)\n",
    "      tokenized_label.extend([labels[i][j]] + [0] * (len(word_tokens) - 1))\n",
    "      \"\"\"\n",
    "      if word in embedding.vocab:\n",
    "        word_seq.extend([embedding.vocab[word].index]+ [0] * (len(word_tokens) - 1))\n",
    "      else:\n",
    "        word_seq.extend([embedding.vocab['unk'].index]+ [0] * (len(word_tokens) - 1))\n",
    "      \"\"\"\n",
    "    tokenized_word.append(\"[SEP]\")\n",
    "    tokenized_label.append(0)\n",
    "    #word_seq.append(0)\n",
    "    if len(tokenized_word) > max_len :\n",
    "        tokenized_word = tokenized_word[: (max_len)]\n",
    "        tokenized_label = tokenized_label[: (max_len)]\n",
    "        #word_seq = word_seq [: (max_len)]\n",
    "    tokenized_sents.append(tokenized_word)\n",
    "    tokenized_labels.append(tokenized_label)\n",
    "    #Fasttext_seq.append(word_seq)\n",
    "\n",
    "  \n",
    "  return tokenized_sents, tokenized_labels#, Fasttext_seq\n",
    "\n",
    "\"\"\"\n",
    "      word_tokens = tokenizer.tokenize(word)\n",
    "      max = 0\n",
    "      index = 0 \n",
    "      for k, t in enumerate (word_tokens):\n",
    "        if (len (t) > max): \n",
    "          max = len (t)\n",
    "          index = k\n",
    "      tokenized_word.extend (word_tokens)\n",
    "      for k, t in enumerate (word_tokens):\n",
    "        if (k == index): \n",
    "          tokenized_label.append(labels[i][j])\n",
    "        else:\n",
    "          tokenized_label.append(0)\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "jfR67ymw4V09"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tr_tokenized_sents , tr_tokenized_labels , tr_Fasttext_seq= encode_sentence(train_sents,train_labels)\n",
    "val_tokenized_sents , val_tokenized_labels, val_Fasttext_seq = encode_sentence(val_sents,val_labels)\n",
    "te_tokenized_sents , te_tokenized_labels, te_Fasttext_seq = encode_sentence(test_sents,test_labels)\n",
    "\"\"\"\n",
    "tr_tokenized_sents , tr_tokenized_labels = encode_sentence(train_sents,train_labels)\n",
    "val_tokenized_sents , val_tokenized_labels = encode_sentence(val_sents,val_labels)\n",
    "te_tokenized_sents , te_tokenized_labels = encode_sentence(test_sents,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XWzdyJXXclmQ",
    "outputId": "36fd5eca-34fc-449e-d758-600e9cbfea0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization result for sent#1:\n",
      "['وهو', 'مع', 'فلسطين', 'خميرة', 'عروبة', 'جديدة']\n",
      "['[CLS]', 'وهو', 'مع', 'فلسطين', 'خم', '##يرة', 'عروبة', 'جديدة', '[SEP]']\n",
      "[0, 'O', 'O', 'B-LOC', 'O', 0, 'O', 'O', 0]\n"
     ]
    }
   ],
   "source": [
    "print ('tokenization result for sent#1:')\n",
    "print (train_sents[11])\n",
    "print (tr_tokenized_sents[11])\n",
    "#print (tokenized_labels[0])\n",
    "print (tr_tokenized_labels[11])\n",
    "\n",
    "\n",
    "#print (tr_Fasttext_seq[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J6QHWaDMnN4g",
    "outputId": "1d4686fe-afa1-4fc6-f68c-23e20bc4c87f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max tokinized sentence length: 177\n",
      "Max tokinized sentence length: 172\n",
      "Max tokinized sentence length: 141\n"
     ]
    }
   ],
   "source": [
    "print('Max tokinized sentence length:',len(max(tr_tokenized_sents, key=len)))\n",
    "print('Max tokinized sentence length:',len(max(te_tokenized_sents, key=len)))\n",
    "print('Max tokinized sentence length:',len(max(val_tokenized_sents, key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "UjHG8MEtWJQD"
   },
   "outputs": [],
   "source": [
    "max_len = len(max(tr_tokenized_sents, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "0RP50XlaeE1V"
   },
   "outputs": [],
   "source": [
    "#---------------get id, mask, and seg -----------------------\n",
    "def sents_processing (sents):\n",
    "  inputs_ids = []\n",
    "  masks = []\n",
    "  seg_ids = []\n",
    "  for sent in sents:\n",
    "    l = max_len-len(sent)\n",
    "    inputs_ids.append (tokenizer.convert_tokens_to_ids([word for word in sent])+ [tokenizer.pad_token_id]*l)\n",
    "    masks.append([1 for word in sent]+ [0]*l)\n",
    "    seg_ids.append([0 for word in sent ]+ [0]*l)\n",
    "\n",
    "  return torch.tensor(inputs_ids, dtype=torch.long), torch.tensor(masks, dtype=torch.long), torch.tensor(seg_ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zDdKWEo_7_BT",
    "outputId": "6b44f228-cbeb-4a43-c4e7-0ef4b70b1835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17028,    12, 25667, 20068, 28764,  8236, 17359,   361, 31529,   883,\n",
      "         1409,  5095, 21174,   731, 27219, 33293, 57166, 26695, 36960, 32010,\n",
      "        47486,  3664,    11, 17030, 17029, 17029, 17029, 17029, 17029, 17029,\n",
      "        17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029,\n",
      "        17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029,\n",
      "        17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029,\n",
      "        17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029,\n",
      "        17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029,\n",
      "        17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029,\n",
      "        17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029,\n",
      "        17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029,\n",
      "        17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029,\n",
      "        17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029,\n",
      "        17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029,\n",
      "        17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029,\n",
      "        17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029,\n",
      "        17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029, 17029,\n",
      "        17029, 17029, 17029, 17029, 17029, 17029, 17029])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "train_ids, train_masks, train_seg = sents_processing (tr_tokenized_sents)\n",
    "val_ids, val_masks, val_seg = sents_processing (val_tokenized_sents)\n",
    "test_ids, test_masks, test_seg = sents_processing (te_tokenized_sents)\n",
    "\n",
    "print (train_ids[0])\n",
    "print (train_masks[0])\n",
    "print (train_seg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "b95GF2XA9EtT"
   },
   "outputs": [],
   "source": [
    "def padding (sents):\n",
    "  Fasttext_seq = []\n",
    "  for sent in sents:\n",
    "    l = max_len-len(sent)\n",
    "    Fasttext_seq.append(sent+ [0]*l)\n",
    "\n",
    "  return torch.tensor(Fasttext_seq, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "j4UqkHID9ljy",
    "outputId": "b69e5e51-9162-4c76-f4cc-ee2058d48eb1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\ntrain_Fasttext_seq = padding (tr_Fasttext_seq)\\ntest_Fasttext_seq = padding (te_Fasttext_seq)\\nval_Fasttext_seq = padding (val_Fasttext_seq)\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_Fasttext_seq = padding (tr_Fasttext_seq)\n",
    "test_Fasttext_seq = padding (te_Fasttext_seq)\n",
    "val_Fasttext_seq = padding (val_Fasttext_seq)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8It99bQGHfTA",
    "outputId": "50bbe7eb-e025-4a81-88e0-e5bc02614925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max inputs_ids length: 177\n"
     ]
    }
   ],
   "source": [
    "print('Max inputs_ids length:',len(max(train_ids, key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "r-tXCFVCupS1"
   },
   "outputs": [],
   "source": [
    "\n",
    "tag2idx = {t: i+1 for i, t in enumerate(tag_classes)}\n",
    "tag2idx[\"PAD\"] = 0\n",
    "idx2tag = {i: w for w, i in tag2idx.items()}\n",
    "def labels_processing (labels):\n",
    "\n",
    "  labels_ids = []\n",
    "  for label in labels:\n",
    "    l = max_len - len (label)\n",
    "    labels_ids.append ([tag2idx[tag] if tag != 0 else tag for tag in label] + [0]*l)\n",
    "  return torch.tensor(labels_ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DGwvzRUe_op5",
    "outputId": "261b739e-bb6c-4d83-ad6d-1f96cf59c28a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "train_labels = labels_processing (tr_tokenized_labels)\n",
    "test_labels = labels_processing (te_tokenized_labels)\n",
    "val_labels = labels_processing (val_tokenized_labels)\n",
    "\n",
    "print (train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IGX1JhbsINIf",
    "outputId": "65b04541-3d20-424d-c00c-c63ea4e7aa44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max inputs_ids length: 177\n"
     ]
    }
   ],
   "source": [
    "print('Max inputs_ids length:',len(max(train_labels, key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "id": "8H_9SsQ6ffud",
    "outputId": "0b53db6d-f019-4c1a-9717-e14eb41e6b52"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\nper = open('PER_gazetteer.txt','r',encoding='utf-8')\\npers_data = per.read().split('\\n')\\nloc = open('LOC_gazetteer.txt','r',encoding='utf-8')\\nloc_data = per.read().split('\\n')\\norg = open('Org_gazetteer.txt','r',encoding='utf-8')\\norg_data = per.read().split('\\n')\\n\\ndef get_gazetteer_featureas (tokenized_sentence):\\n  gazetteer_featureas = []\\n  for sent in tokenized_sentence:\\n    seq = []\\n    for word in sent:\\n      if word in pers_data:\\n        seq.append (1)\\n      elif word in loc_data:\\n        seq.append (2)\\n      elif word in org_data:\\n        seq.append (3)\\n      else:\\n        seq.append (0) \\n    gazetteer_featureas.append(seq)\\n  return (gazetteer_featureas)   \\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----------------------gazetteer featureas--------------------\n",
    "\"\"\"\n",
    "per = open('PER_gazetteer.txt','r',encoding='utf-8')\n",
    "pers_data = per.read().split('\\n')\n",
    "loc = open('LOC_gazetteer.txt','r',encoding='utf-8')\n",
    "loc_data = per.read().split('\\n')\n",
    "org = open('Org_gazetteer.txt','r',encoding='utf-8')\n",
    "org_data = per.read().split('\\n')\n",
    "\n",
    "def get_gazetteer_featureas (tokenized_sentence):\n",
    "  gazetteer_featureas = []\n",
    "  for sent in tokenized_sentence:\n",
    "    seq = []\n",
    "    for word in sent:\n",
    "      if word in pers_data:\n",
    "        seq.append (1)\n",
    "      elif word in loc_data:\n",
    "        seq.append (2)\n",
    "      elif word in org_data:\n",
    "        seq.append (3)\n",
    "      else:\n",
    "        seq.append (0) \n",
    "    gazetteer_featureas.append(seq)\n",
    "  return (gazetteer_featureas)   \n",
    "\"\"\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "-XH2Pi3_hgjW"
   },
   "outputs": [],
   "source": [
    "#train_gazetteer_featureas = get_gazetteer_featureas (tr_tokenized_sents) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "a01BjGnOoe1a"
   },
   "outputs": [],
   "source": [
    "#-------------- char embedding processing ------------------\n",
    "char2idx = {\"PAD\": 0, \"UNK\": 1}\n",
    "for c in \"0123456789ABCDEFGHIJKLMNOPRSTUVWXYZabcdefghiklmnopqrstuvwxyíüءآأؤإئابةتثجحخدذرزسشصضطظعغفقكلمنهوىي–“”…ﻷﻹ\":\n",
    "  char2idx[c] = len(char2idx)\n",
    "max_len_char = 20\n",
    "def get_char_ids (sents):\n",
    "\n",
    "  chars_ids = []\n",
    "  for sent in sents:\n",
    "    sent_char_seq = []\n",
    "    for word in sent:\n",
    "      word_char_seq = []       \n",
    "      for char in word:\n",
    "        if char in char2idx:\n",
    "          word_char_seq.append (char2idx[char])\n",
    "        else:\n",
    "          word_char_seq.append (char2idx['UNK'])\n",
    "      lw = max_len_char - len(word)\n",
    "      word_char_seq += [0]*lw\n",
    "      sent_char_seq.append(word_char_seq)\n",
    "    l = max_len-len(sent)\n",
    "    sent_char_seq += [[0]*max_len_char]*l\n",
    "    chars_ids.append(sent_char_seq)\n",
    "  return torch.tensor(chars_ids, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "5DYLvZxrrYnd"
   },
   "outputs": [],
   "source": [
    "train_chars = get_char_ids (tr_tokenized_sents)\n",
    "val_chars = get_char_ids (val_tokenized_sents)\n",
    "test_chars = get_char_ids (te_tokenized_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9jq1Yv25lRV",
    "outputId": "80a6aeca-aaf2-4f28-a947-d48f3747d755"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'ولم', 'يعد', 'من', 'الممكن', 'تبرير', 'سلوك', 'القوى', 'السياسية', 'الشيعية', 'للعرب', 'والمسلمين', 'لا', 'من', 'حيث', 'تحالف', 'هذه', 'القوى', 'مع', 'الاحتلال', 'وتصد', '##يها', 'للمقاومة', 'ولا', 'من', 'حيث', 'سياستها', 'وسلوك', '##ها', 'الطائفي', '[SEP]']\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "['ولم', 'يعد', 'من', 'الممكن', 'تبرير', 'سلوك', 'القوى', 'السياسية', 'الشيعية', 'للعرب', 'والمسلمين', 'لا', 'من', 'حيث', 'تحالف', 'هذه', 'القوى', 'مع', 'الاحتلال', 'وتصديها', 'للمقاومة', 'ولا', 'من', 'حيث', 'سياستها', 'وسلوكها', 'الطائفي']\n"
     ]
    }
   ],
   "source": [
    "print (te_tokenized_sents[0])\n",
    "print (test_labels[0])\n",
    "print (test_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "U2UP1mya662H"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "train_batch_size = 32\n",
    "test_batch_size = 8\n",
    "\n",
    "# wrap tensors\n",
    "#train_data = TensorDataset(train_ids, train_masks, train_seg, train_Fasttext_seq, train_labels)\n",
    "train_data = TensorDataset(train_ids, train_masks, train_seg, train_labels)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, batch_size=train_batch_size, num_workers=2)\n",
    "\n",
    "# wrap tensors\n",
    "#val_data = TensorDataset(val_ids, val_masks, val_seg, val_Fasttext_seq, val_labels)\n",
    "val_data = TensorDataset(val_ids, val_masks, val_seg, val_labels)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, batch_size=test_batch_size, num_workers=1)\n",
    "\n",
    "# wrap tensors\n",
    "#test_data = TensorDataset(test_ids, test_masks, test_seg, test_Fasttext_seq, test_labels)\n",
    "test_data = TensorDataset(test_ids, test_masks, test_seg, test_labels)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "test_dataloader = DataLoader(test_data, batch_size=test_batch_size, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "8af805fa7c034b6a8bdb7884f108eb92",
      "3007de8871f84c618d1f16afcfa7c3f6",
      "56049dbc56d34d69846583d56965e868",
      "ca91fb6928644654bf443a7a7285a496",
      "ba564380304345f1b6b33e9d8cd76008",
      "8323c32e61684bdd90a4d49eb95f8f39",
      "d7c083ce1ca8438bb6b1799431da1745",
      "42f5af72d9604a9da974471c25c2d862"
     ]
    },
    "id": "izMzl4vh6HY-",
    "outputId": "02c195a1-06e6-41dc-a3ea-516ab505ca67"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af805fa7c034b6a8bdb7884f108eb92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=543450723.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertConfig, AdamW\n",
    " \n",
    "bert = BertModel.from_pretrained(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "jqnbxnmL6m_L"
   },
   "outputs": [],
   "source": [
    "class BERTNERTagger(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert \n",
    "\n",
    "        self.gruUnits = 384\n",
    "      \n",
    "      # dropout layer\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        self.bidirectional = True\n",
    "      # dense layer 1\n",
    "        self.gru = nn.GRU(768, self.gruUnits,  bidirectional = self.bidirectional)\n",
    "        \n",
    "        self.fc = nn.Linear(self.gruUnits * 2 if self.bidirectional else self.gruUnits, len(tag2idx))\n",
    "        #self.fc = nn.Linear(768, len(tag2idx))\n",
    "\n",
    "\n",
    "      #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "                embed_dim=self.gruUnits * 2,\n",
    "                num_heads=4,\n",
    "                dropout=0.5\n",
    "            )\n",
    "        #self.embedding = nn.Embedding.from_pretrained(weights, padding_idx = 0)\n",
    "        #self.embedding.weight.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_id,masks, seg):\n",
    "\n",
    "        pool,seq = self.bert(input_id, attention_mask=masks, token_type_ids=seg)\n",
    "\n",
    "        pool = self.dropout (pool)\n",
    "\n",
    "        #fast = self.embedding (Fasttext_seq)\n",
    "        \n",
    "        pool , h = self.gru(pool)\n",
    "        pool = self.dropout (pool)\n",
    "\n",
    "        #combined = self.dropout (combined)\n",
    "\n",
    "      # output layer\n",
    "        predictions = self.fc(pool)\n",
    "        \"\"\"\n",
    "      \n",
    "        log_likelihood = self.crf(logits, tags, mask)\n",
    "        loss = -log_likelihood # Log likelihood is not normalized (It is not divided by the batch size).\n",
    "\n",
    "        # To obtain the best sequence using viterbi decoding\n",
    "        best_tag_sequence = self.crf.best_viterbi_tag(logits, mask)\n",
    "\n",
    "        # To obtain output similar to the lstm prediction we can use the below code\n",
    "        predictions = logits * 0.0\n",
    "        for i, instance_tags in enumerate(best_tag_sequence):\n",
    "            for j, tag_id in enumerate(instance_tags[0][0]):\n",
    "                predictions[i, j, int(tag_id)] = 1\n",
    "      # apply softmax activation\n",
    "        #predictions = self.softmax(x)\n",
    "        \"\"\"\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "NuVMQzpwGnGh"
   },
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERTNERTagger(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z0Zck_R_G75z",
    "outputId": "5a2ba437-e2a9-441c-8c4c-a6e66d0b5fe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training steps:  922\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 5e-5\n",
    "WARMUP_RATIO = 0.1\n",
    "MAX_GRAD_NORM = 1.0\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.01,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "num_train_steps = int(len(train_sents) / train_batch_size * EPOCHS)\n",
    "print('Number of training steps: ', num_train_steps)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "#optimizer = AdamW(optimizer_parameters, lr=LEARNING_RATE)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=int(WARMUP_RATIO*num_train_steps), num_training_steps=num_train_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "P6DSJ1PROikI"
   },
   "outputs": [],
   "source": [
    "def loss_fn(output, target, mask, num_labels):\n",
    "    lfn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    active_loss = mask.view(-1) == 1\n",
    "    active_logits = output.view(-1, num_labels)\n",
    "    active_labels = torch.where(\n",
    "        active_loss,\n",
    "        target.view(-1),\n",
    "        torch.tensor(lfn.ignore_index).type_as(target)\n",
    "    )\n",
    "    loss = lfn(active_logits, active_labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "08CajfceFu9M"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, classification_report, accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "\n",
    "def align_predictions(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "    batch_size, seq_len = preds.shape\n",
    "\n",
    "    out_label_list = [[] for _ in range(batch_size)]\n",
    "    preds_list = [[] for _ in range(batch_size)]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            if label_ids[i, j] != 0:\n",
    "                out_label_list[i].append(idx2tag[label_ids[i][j]])\n",
    "                preds_list[i].append(idx2tag[preds[i][j]])\n",
    "    return preds_list, out_label_list\n",
    "\n",
    "def y2label(zipped, mask=0):\n",
    "    out_true = []\n",
    "    out_pred = []\n",
    "    for zip_i in zipped:\n",
    "        a, b = tuple(zip_i)\n",
    "        if a != mask :\n",
    "            out_true.append(idx2tag[a].replace('B-','').replace('I-',''))\n",
    "            out_pred.append(idx2tag[b].replace('B-','').replace('I-',''))\n",
    "    return out_true, out_pred\n",
    "\n",
    "def compute_metrics(predictions,label_ids):\n",
    "    print (\"predictions:\" , predictions.shape)\n",
    "    print (\"label_ids:\" , label_ids.shape)\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    y_zipped = zip(np.array(label_ids).flat, np.array(predictions).flat)\n",
    "    preds_list, out_label_list = y2label(y_zipped)\n",
    "    print(classification_report(out_label_list, preds_list,digits=4))\n",
    "    return {\n",
    "        \"accuracy_score\": accuracy_score(out_label_list, preds_list),\n",
    "        \"precision\": precision_score(out_label_list, preds_list, average='macro'),\n",
    "        \"recall\": recall_score(out_label_list, preds_list, average='macro'),\n",
    "        \"f1\": f1_score(out_label_list, preds_list, average='macro'),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "liLdb1uxNb85"
   },
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "  \n",
    "  model.train()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save model predictions\n",
    "  total_preds=[]\n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "    # progress update after every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [r.to(device) for r in batch]\n",
    " \n",
    "    #sent_id, mask, seg, fast, labels = batch\n",
    "    sent_id, mask, seg, labels = batch\n",
    "\n",
    "    # clear previously calculated gradients \n",
    "    optimizer.zero_grad()        \n",
    "    # get model predictions for the current batch\n",
    "    preds = model(sent_id, mask, seg)\n",
    "    # compute the loss between actual and predicted values\n",
    "    # add on to the total loss\n",
    "    loss = loss_fn(preds, labels, mask, len(tag2idx))\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "    # backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    # model predictions are stored on GPU. So, push it to CPU\n",
    "    preds=preds.detach().cpu().numpy()\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds)\n",
    "\n",
    "  # compute the training loss of the epoch\n",
    "  avg_loss = total_loss / len(train_dataloader)\n",
    "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  #returns the loss and predictions\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "v9SlCYtgTWF9"
   },
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate(dataloader):\n",
    "  \n",
    "  print(\"\\nEvaluating...\")\n",
    "  \n",
    "  # deactivate dropout layers\n",
    "  model.eval()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save the model predictions\n",
    "  total_preds = []\n",
    "  total_labels = []\n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(dataloader):\n",
    "    \n",
    "    # Progress update every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "            \n",
    "      # Report progress.\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [t.to(device) for t in batch]\n",
    "\n",
    "    sent_id, mask, seg, labels = batch\n",
    "\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "      \n",
    "      # model predictions\n",
    "      preds= model(sent_id, mask, seg)\n",
    "      # compute the validation loss between actual and predicted values\n",
    "      loss = loss_fn(preds, labels, mask, len(tag2idx))\n",
    "      total_loss = total_loss + loss  \n",
    "      preds = preds.detach().cpu().numpy()\n",
    "      labels=labels.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "      total_preds.append(preds)\n",
    "      total_labels.append(labels)\n",
    "  # compute the validation loss of the epoch\n",
    "  avg_loss = total_loss / len(dataloader) \n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "  total_labels  = np.concatenate(total_labels, axis=0)\n",
    "\n",
    "  return avg_loss, total_preds, compute_metrics(total_preds,total_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LUyOvaQyQFXr",
    "outputId": "f3af495e-9cf6-41c6-efb3-8b83572ccd94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 5\n",
      "  Batch    50  of    185.\n",
      "  Batch   100  of    185.\n",
      "  Batch   150  of    185.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     84.\n",
      "predictions: (665, 177, 10)\n",
      "label_ids: (665, 177)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC     0.9365    0.6987    0.8003       823\n",
      "        MISC     0.6785    0.7846    0.7277       390\n",
      "           O     0.9674    0.9909    0.9790     15743\n",
      "         ORG     0.8375    0.5255    0.6458       510\n",
      "        PERS     0.9533    0.9045    0.9283       880\n",
      "\n",
      "    accuracy                         0.9563     18346\n",
      "   macro avg     0.8746    0.7808    0.8162     18346\n",
      "weighted avg     0.9556    0.9563    0.9540     18346\n",
      "\n",
      "\n",
      "Training Loss: 0.363\n",
      "Validation Loss: 0.166\n",
      "{'accuracy_score': 0.9563392565136815, 'precision': 0.87462992102464, 'recall': 0.7808462119161483, 'f1': 0.8162108599595739}\n",
      "\n",
      " Epoch 2 / 5\n",
      "  Batch    50  of    185.\n",
      "  Batch   100  of    185.\n",
      "  Batch   150  of    185.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     84.\n",
      "predictions: (665, 177, 10)\n",
      "label_ids: (665, 177)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC     0.9349    0.7694    0.8441       746\n",
      "        MISC     0.7805    0.7258    0.7521       485\n",
      "           O     0.9699    0.9935    0.9815     15742\n",
      "         ORG     0.8656    0.6035    0.7112       459\n",
      "        PERS     0.9701    0.8862    0.9262       914\n",
      "\n",
      "    accuracy                         0.9622     18346\n",
      "   macro avg     0.9042    0.7957    0.8430     18346\n",
      "weighted avg     0.9608    0.9622    0.9604     18346\n",
      "\n",
      "\n",
      "Training Loss: 0.093\n",
      "Validation Loss: 0.147\n",
      "{'accuracy_score': 0.9622260983320615, 'precision': 0.9041776878863648, 'recall': 0.795686198469388, 'f1': 0.8430430040859234}\n",
      "\n",
      " Epoch 3 / 5\n",
      "  Batch    50  of    185.\n",
      "  Batch   100  of    185.\n",
      "  Batch   150  of    185.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     84.\n",
      "predictions: (665, 177, 10)\n",
      "label_ids: (665, 177)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC     0.9365    0.8370    0.8839       687\n",
      "        MISC     0.7871    0.7618    0.7743       466\n",
      "           O     0.9808    0.9922    0.9865     15941\n",
      "         ORG     0.8531    0.7299    0.7867       374\n",
      "        PERS     0.9689    0.9214    0.9445       878\n",
      "\n",
      "    accuracy                         0.9718     18346\n",
      "   macro avg     0.9053    0.8485    0.8752     18346\n",
      "weighted avg     0.9711    0.9718    0.9712     18346\n",
      "\n",
      "\n",
      "Training Loss: 0.053\n",
      "Validation Loss: 0.131\n",
      "{'accuracy_score': 0.9718194701842363, 'precision': 0.9052894894674243, 'recall': 0.8484710118961061, 'f1': 0.8751963231329809}\n",
      "\n",
      " Epoch 4 / 5\n",
      "  Batch    50  of    185.\n",
      "  Batch   100  of    185.\n",
      "  Batch   150  of    185.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     84.\n",
      "predictions: (665, 177, 10)\n",
      "label_ids: (665, 177)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC     0.9446    0.8274    0.8821       701\n",
      "        MISC     0.8160    0.7667    0.7905       480\n",
      "           O     0.9806    0.9930    0.9867     15925\n",
      "         ORG     0.8469    0.7384    0.7889       367\n",
      "        PERS     0.9629    0.9210    0.9415       873\n",
      "\n",
      "    accuracy                         0.9722     18346\n",
      "   macro avg     0.9102    0.8493    0.8780     18346\n",
      "weighted avg     0.9714    0.9722    0.9715     18346\n",
      "\n",
      "\n",
      "Training Loss: 0.031\n",
      "Validation Loss: 0.121\n",
      "{'accuracy_score': 0.9722010247465388, 'precision': 0.9101859065861422, 'recall': 0.8492809922253901, 'f1': 0.8779612717202344}\n",
      "\n",
      " Epoch 5 / 5\n",
      "  Batch    50  of    185.\n",
      "  Batch   100  of    185.\n",
      "  Batch   150  of    185.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     84.\n",
      "predictions: (665, 177, 10)\n",
      "label_ids: (665, 177)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC     0.9267    0.8891    0.9075       640\n",
      "        MISC     0.8404    0.7735    0.8055       490\n",
      "           O     0.9856    0.9924    0.9890     16014\n",
      "         ORG     0.8844    0.8040    0.8423       352\n",
      "        PERS     0.9557    0.9388    0.9472       850\n",
      "\n",
      "    accuracy                         0.9769     18346\n",
      "   macro avg     0.9185    0.8796    0.8983     18346\n",
      "weighted avg     0.9763    0.9769    0.9765     18346\n",
      "\n",
      "\n",
      "Training Loss: 0.021\n",
      "Validation Loss: 0.114\n",
      "{'accuracy_score': 0.9768886950833969, 'precision': 0.918535954259944, 'recall': 0.8795553602593322, 'f1': 0.8982901300343208}\n"
     ]
    }
   ],
   "source": [
    "epochs = EPOCHS\n",
    "MODEL_PATH = 'BERT_BGRU_softmax(ANERCorp_+_AQMAR).bin'\n",
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "best_f1 = 0\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "   \n",
    "    #evaluate model\n",
    "    valid_loss, preds, eval_metrics = evaluate(val_dataloader)\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')\n",
    "    print (eval_metrics)\n",
    "    if eval_metrics['f1'] > best_f1:\n",
    "      torch.save(model, MODEL_PATH)\n",
    "      best_f1 = eval_metrics['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NZl0SZmFTRQA",
    "outputId": "7031bcf4-d73d-48f0-cd39-406d8ce19ad1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n",
      "  Batch    50  of     92.\n",
      "predictions: (729, 177, 10)\n",
      "label_ids: (729, 177)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC     0.9230    0.9256    0.9243       699\n",
      "        MISC     0.8581    0.7741    0.8140       633\n",
      "           O     0.9872    0.9917    0.9894     18060\n",
      "         ORG     0.8640    0.8604    0.8622       480\n",
      "        PERS     0.9676    0.9480    0.9577      1038\n",
      "\n",
      "    accuracy                         0.9777     20910\n",
      "   macro avg     0.9200    0.9000    0.9095     20910\n",
      "weighted avg     0.9773    0.9777    0.9774     20910\n",
      "\n",
      "Test Loss = 0.10893362760543823\n",
      "{'accuracy_score': 0.9777140124342419, 'precision': 0.9199673475383315, 'recall': 0.8999575072111938, 'f1': 0.909507429220372}\n"
     ]
    }
   ],
   "source": [
    "#model.load_state_dict(torch.load(MODEL_PATH))\n",
    "test_loss, preds, test_metrics = evaluate (test_dataloader)\n",
    "print(f\"Test Loss = {test_loss}\")\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "bL6gUvlvR4_z"
   },
   "outputs": [],
   "source": [
    "test_sentence = \"وقد حصل الفائزون على جوائز عينية من شركة سعادة للمواد الكهربائية\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L3uxGSNzR5wj",
    "outputId": "dbea8b93-29a7-4bdc-dc9c-661e795eb622"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['وقد', 'حصل', 'الفائزون', 'على', 'جوائز', 'عينية', 'من', 'شركة', 'سعادة', 'للمواد', 'الكهربائية']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = test_sentence.split (\" \")\n",
    "print (test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "piaCIq_7rzea",
    "outputId": "878f96a4-4b75-4e19-ac18-ad60af2f80f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[CLS]', 'وقد', 'حصل', 'الفائز', '##ون', 'على', 'جوائز', 'عينية', 'من', 'شركة', 'سعادة', 'للمواد', 'الكهربائية', '[SEP]']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_test_sentence, _ = encode_sentence([test_sentence],[test_sentence])\n",
    "print (tokenized_test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "aotiaVsnsNwW"
   },
   "outputs": [],
   "source": [
    "test_sentence_ids, test_sentence_masks, test_sentence_seg = sents_processing (tokenized_test_sentence)#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bmfiUJ7R8fv",
    "outputId": "3ea2404f-2e0a-4d0b-9fe7-f5548c48f692"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "وقد O\n",
      "حصل O\n",
      "الفائز O\n",
      "##ون O\n",
      "على O\n",
      "جوائز O\n",
      "عينية O\n",
      "من O\n",
      "شركة O\n",
      "سعادة B-ORG\n",
      "للمواد O\n",
      "الكهربائية O\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(test_sentence_ids.cuda(), test_sentence_masks.cuda(), test_sentence_seg.cuda())\n",
    "output = output[0].detach().cpu().numpy()\n",
    "tokens = tokenizer.convert_ids_to_tokens(test_sentence_ids.to('cpu').numpy()[0])\n",
    "output = np.argmax(output, axis=1)\n",
    "for i , tag in enumerate (output):\n",
    "  if tokens[i] != '[PAD]' and tokens[i] != '[CLS]' and tokens[i] != '[SEP]':\n",
    "    print (tokens[i], idx2tag[tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "RQdDoKdDdAXp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "id": "DVgPzjzkbYdU",
    "outputId": "e67b0b24-726d-4c2b-f4f9-3f8a17eb6bfe"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-f957d2c9f2c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tags' is not defined"
     ]
    }
   ],
   "source": [
    "print (tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clurvVnWbdTm"
   },
   "outputs": [],
   "source": [
    "print (tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWScT2reYwts"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ANERCorp+AQMAR_run1(80,10,10)(384_0_2).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0d76b4f0d64a4b83a6c129c2bb2d6dd6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c51d1d5c7144a8e905946f60cc28b0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ab439891e0046cea393cea902283a40",
      "max": 780030,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bd5e04057060457882459661e3277830",
      "value": 780030
     }
    },
    "3007de8871f84c618d1f16afcfa7c3f6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30ee8c72ceda4b969afbb6743d2d254e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6ee106c686ae403a87bf69dd671ef59e",
       "IPY_MODEL_a8e5fe7736e6469d93aaddc875900087"
      ],
      "layout": "IPY_MODEL_3398b97e49de4f74b50911fe8fbdf18d"
     }
    },
    "3398b97e49de4f74b50911fe8fbdf18d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ab439891e0046cea393cea902283a40": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42f5af72d9604a9da974471c25c2d862": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56049dbc56d34d69846583d56965e868": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8323c32e61684bdd90a4d49eb95f8f39",
      "max": 543450723,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ba564380304345f1b6b33e9d8cd76008",
      "value": 543450723
     }
    },
    "5a198de7545e4bf091d9c30492304088": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6ee106c686ae403a87bf69dd671ef59e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f5d00c478fce408e93e217c0bac88c5f",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_accea898bce140eb87e648a4bd437bac",
      "value": 570
     }
    },
    "8323c32e61684bdd90a4d49eb95f8f39": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8af805fa7c034b6a8bdb7884f108eb92": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_56049dbc56d34d69846583d56965e868",
       "IPY_MODEL_ca91fb6928644654bf443a7a7285a496"
      ],
      "layout": "IPY_MODEL_3007de8871f84c618d1f16afcfa7c3f6"
     }
    },
    "a8e5fe7736e6469d93aaddc875900087": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd0c20fc35bb49f18302c97785df0fff",
      "placeholder": "​",
      "style": "IPY_MODEL_b1175768036e4d94b1eee31a14654e9b",
      "value": " 570/570 [00:00&lt;00:00, 707B/s]"
     }
    },
    "accea898bce140eb87e648a4bd437bac": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ad53b5be46e54a218ca8807661b5d7bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1c51d1d5c7144a8e905946f60cc28b0d",
       "IPY_MODEL_dafa709a606a41249c7534f15d678f6c"
      ],
      "layout": "IPY_MODEL_0d76b4f0d64a4b83a6c129c2bb2d6dd6"
     }
    },
    "b1175768036e4d94b1eee31a14654e9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba564380304345f1b6b33e9d8cd76008": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "bd5e04057060457882459661e3277830": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ca91fb6928644654bf443a7a7285a496": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_42f5af72d9604a9da974471c25c2d862",
      "placeholder": "​",
      "style": "IPY_MODEL_d7c083ce1ca8438bb6b1799431da1745",
      "value": " 543M/543M [00:23&lt;00:00, 23.3MB/s]"
     }
    },
    "d5fa2aa6a9db4fd6b946e1a6e59270ce": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7c083ce1ca8438bb6b1799431da1745": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dafa709a606a41249c7534f15d678f6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5fa2aa6a9db4fd6b946e1a6e59270ce",
      "placeholder": "​",
      "style": "IPY_MODEL_5a198de7545e4bf091d9c30492304088",
      "value": " 780k/780k [00:02&lt;00:00, 375kB/s]"
     }
    },
    "f5d00c478fce408e93e217c0bac88c5f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd0c20fc35bb49f18302c97785df0fff": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
